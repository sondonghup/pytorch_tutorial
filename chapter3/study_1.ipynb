{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(2,2)\n",
    "print(x)\n",
    "x = torch.Tensor([[1, 2], [3, 4]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4]]\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "x = [[1, 2], [3, 4]]\n",
    "print(x)\n",
    "x = np.array(x)\n",
    "print(x)\n",
    "x = torch.from_numpy(x)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1., 3.],\n",
      "        [5., 7.]])\n",
      "tensor([[1., 3.],\n",
      "        [5., 7.]], requires_grad=True)\n",
      "tensor([[ 3.,  9.],\n",
      "        [11., 14.]], grad_fn=<AddBackward0>)\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1., 3.],\n",
      "        [5., 7.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(x)\n",
    "y = torch.FloatTensor([[1, 3], [5, 7]])\n",
    "print(y)\n",
    "y.requires_grad_(True)\n",
    "print(y)\n",
    "z = (x + y) + torch.FloatTensor([[1, 4], [3, 3]])\n",
    "print(z)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1., 3.],\n",
      "        [5., 7.]])\n",
      "tensor([[1., 3.],\n",
      "        [5., 7.]], requires_grad=True)\n",
      "tensor([[ 3.,  9.],\n",
      "        [11., 14.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(x)\n",
    "y = torch.FloatTensor([[1, 3], [5, 7]])\n",
    "print(y)\n",
    "y.requires_grad_(True)\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = (x + y) + torch.FloatTensor([[1, 4], [3, 3]])\n",
    "    print(z)\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15., 22.],\n",
      "        [23., 33.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2, 3], [2, 3, 4]])\n",
    "y = torch.FloatTensor([[1, 2], [2, 3], [3, 4]])\n",
    "z = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "\n",
    "def linear(x, y, z):\n",
    "    result = torch.mm(x, y) + z\n",
    "    return result\n",
    "\n",
    "print(linear(x, y, z))\n",
    "# mm -> matrix multiplication\n",
    "# bmm -> batch matrix multiplication\n",
    "# matmul -> matrix, batch matrix multiplication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 6.6021e-07, 1.7185e-04],\n",
      "        [1.9635e-10, 1.9979e-10, 6.6041e-07, 1.7185e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 6.6021e-07, 1.7185e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 6.6021e-07, 1.7185e-04]])\n"
     ]
    }
   ],
   "source": [
    "class mylinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w = torch.FloatTensor(input_size, output_size)\n",
    "        self.b = torch.FloatTensor(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.mm(x, self.w) + self.b\n",
    "        \n",
    "linear = mylinear(2, 4)\n",
    "result = linear(torch.FloatTensor(4, 2))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "params = [p for p in linear.parameters()]\n",
    "print(params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빈 배열이 나오는 이유는 학습 가능한 파라미터가 없기 때문\n",
    "-> parameter객체로 텐서를 감싸야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.b Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.3221e+13, 4.3370e+28,        inf,        inf],\n",
      "        [6.4999e-21, 1.4923e-09, 1.2102e+19, 3.3221e+13],\n",
      "        [1.6546e-18, 1.2136e-07, 9.8422e+20, 2.7017e+15]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class mylinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(mylinear, self).__init__()\n",
    "\n",
    "        self.w = nn.Parameter(torch.FloatTensor(input_size, output_size))\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_size))\n",
    "        print('self.b', self.b)\n",
    "    def forward(self, x):\n",
    "        return torch.mm(x, self.w) + self.b\n",
    "\n",
    "linear = mylinear(2, 4)\n",
    "result = linear(torch.FloatTensor(4, 2))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[28., 39.],\n",
      "        [36., 51.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class mylinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(mylinear, self).__init__()\n",
    "\n",
    "        self.w = nn.Parameter(torch.FloatTensor([[1, 2], [3, 4]]))\n",
    "        self.b = nn.Parameter(torch.FloatTensor([[5, 5], [5, 5]]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.mm(x, self.w) + self.b\n",
    "\n",
    "linear = mylinear(2, 4)\n",
    "result = linear(torch.FloatTensor([[5, 6], [7, 8]]))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True), Parameter containing:\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "params = [p for p in linear.parameters()]\n",
    "print(params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward 역전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(10816., grad_fn=<PowBackward0>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "objective = 50\n",
    "\n",
    "loss = (objective - result.sum())**2\n",
    "print('loss', loss)\n",
    "print(loss.backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.3614e-01,  2.5292e-01, -4.3508e-01, -4.7283e-01],\n",
      "        [ 6.3614e-01,  2.5292e-01, -4.3508e-01, -4.7283e-01],\n",
      "        [ 6.3614e-01,  2.5292e-01, -4.3508e-01, -4.7283e-01],\n",
      "        [-1.4185e+20, -7.0060e+18,  1.2350e+20, -7.7721e+19]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(2, 4)\n",
    "result = linear(torch.FloatTensor(4, 2))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mymodel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Mymodel, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_size, output_size, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def ground_truth(x):\n",
    "    return 3 * x[:, 0] + x[:, 1] - 2 * x [:, 2]\n",
    "\n",
    "def train(model, x, y, optim):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    y_hat = model(x)\n",
    "\n",
    "    loss = ((y - y_hat)**2).sum() / x.size(0)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mymodel(\n",
      "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "n_epochs = 1000\n",
    "n_iter = 10000\n",
    "\n",
    "model = Mymodel(3, 1)\n",
    "optim = torch.optim.SGD(model.parameters(), lr = 0.0001, momentum = 0.1)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3989) tensor([0.9000]) tensor([[0.6891]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.2786) tensor([0.9000]) tensor([[0.7294]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.1897) tensor([0.9000]) tensor([[0.7621]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.1344) tensor([0.9000]) tensor([[0.7852]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0915) tensor([0.9000]) tensor([[0.8120]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0626) tensor([0.9000]) tensor([[0.8264]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0429) tensor([0.9000]) tensor([[0.8416]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0301) tensor([0.9000]) tensor([[0.8561]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0207) tensor([0.9000]) tensor([[0.8646]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0142) tensor([0.9000]) tensor([[0.8712]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0098) tensor([0.9000]) tensor([[0.8790]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0068) tensor([0.9000]) tensor([[0.8842]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0047) tensor([0.9000]) tensor([[0.8875]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0033) tensor([0.9000]) tensor([[0.8913]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0022) tensor([0.9000]) tensor([[0.8942]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0016) tensor([0.9000]) tensor([[0.8960]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0011) tensor([0.9000]) tensor([[0.8973]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.0007) tensor([0.9000]) tensor([[0.8989]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    avg_loss = 0\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        x = torch.rand(batch_size, 3)\n",
    "        y = ground_truth(x.data)\n",
    "\n",
    "        loss = train(model, x, y, optim)\n",
    "\n",
    "        avg_loss += loss\n",
    "    avg_loss = avg_loss/n_iter\n",
    "\n",
    "    x_valid = torch.FloatTensor([[.3, .2, .1]])\n",
    "    y_valid = ground_truth(x_valid.data)\n",
    "\n",
    "    model.eval()\n",
    "    y_hat = model(x_valid)\n",
    "    model.train()\n",
    "\n",
    "    print(avg_loss, y_valid, y_hat)\n",
    "\n",
    "    if avg_loss < 0.001:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
